{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2 - Data From The Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Web Scraping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first write all functions needed to extract the data from the websites and demonstrate how they work. Then we will complete the assignment using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "\n",
    "TOP_UNI_URL = 'https://www.topuniversities.com'\n",
    "TIMES_URL = 'https://www.timeshighereducation.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Top Universities Website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a quick look with Postman on the webside, we realised that the ranking table data were not given on the page but retrived and added with a script on a json file. We then had to directly get this file to obtain the information we need for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TOP_WORLD_RANKING_FILE = TOP_UNI_URL + '/sites/default/files/qs-rankings-data/357051.txt'\n",
    "ranking_top_uni = rq.get(TOP_WORLD_RANKING_FILE).json().get('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file returned by the request is a simple json object with the list of universities ordered by rank after the 'Data' tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_unis = []\n",
    "for uni in ranking_top_uni[:200]:\n",
    "    top_unis.append({'Name' : uni.get('title'), 'Rank' : uni.get('rank_display').replace('=', ''), \n",
    "                     'Country' : uni.get('country'), 'Region' : uni.get('region'), 'Url' : uni.get('url')})\n",
    "print(top_unis[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the functions to obtain the amount of faculty members (total & international) and students (total & international)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_num_members(soup):\n",
    "    return int(soup.find('div', class_='total faculty').find('div', class_='number').text.strip().replace(',', '')),\\\n",
    "           int(soup.find('div', class_='inter faculty').find('div', class_='number').text.strip().replace(',', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = rq.get(TOP_UNI_URL + '/universities/ecole-polytechnique-fédérale-de-lausanne-epfl')\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "tot, inter = get_num_members(soup)\n",
    "print('EPFL number of faculty members (total, international:)', tot, inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_num_students(soup):\n",
    "    return int(soup.find('div', class_='total student').find('div', class_='number').text.strip().replace(',', '')),\\\n",
    "           int(soup.find('div', class_='total inter').find('div', class_='number').text.strip().replace(',', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot, inter = get_num_students(soup)\n",
    "print('EPFL number of students (total, international:', tot, inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "problematic_uni = []\n",
    "for uni in top_unis:\n",
    "    page = rq.get(TOP_UNI_URL + uni['Url'])\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    try:\n",
    "        tot_mem, inter_mem = get_num_members(soup)\n",
    "        tot_stud, inter_stud = get_num_students(soup)\n",
    "        uni['Tot_Mem'] = tot_mem\n",
    "        uni['Inter_Mem'] = inter_mem\n",
    "        uni['Tot_Stud'] = tot_stud\n",
    "        uni['Inter_Stud'] = inter_stud\n",
    "    except AttributeError: \n",
    "        problematic_uni.append(top_unis.index(uni))\n",
    "        print('Could not fetch data from:', uni['Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see two universities don't have the data on the topuniversities website so we will add them by hand watching on the website our self."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nyu = top_unis[problematic_uni[0]]\n",
    "nyu['Tot_Mem'] = 7717\n",
    "nyu['Inter_Mem'] = 604\n",
    "nyu['Tot_Stud'] = 43860\n",
    "nyu['Inter_Stud'] = 11593\n",
    "\n",
    "bang = top_unis[problematic_uni[1]]\n",
    "bang['Tot_Mem'] = 423\n",
    "bang['Inter_Mem'] = 0\n",
    "bang['Tot_Stud'] = 4071\n",
    "bang['Inter_Stud'] = 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(top_unis[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will save those data in a pickle file to be able to use them without requesting everytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('topunis.pickle', 'wb') as out:\n",
    "    pickle.dump(top_unis, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_uni_df = pd.DataFrame.from_dict(top_unis).set_index('Rank').drop('Url', axis=1)\n",
    "top_uni_df = top_uni_df[['Name', 'Country', 'Region', 'Tot_Stud', 'Inter_Stud', 'Tot_Mem', 'Inter_Mem']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_uni_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Times Higher Education Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_TIMES_RANKING_FILE = TIMES_URL + '/sites/default/files/the_data_rankings/world_university_rankings_2018_limit0_369a9045a203e176392b9fb8f8c1cb2a.json'\n",
    "ranking_top_times = rq.get(TOP_TIMES_RANKING_FILE).json().get('data')\n",
    "for key in ranking_top_times[2].keys(): \n",
    "    print(\"{}: {}\". format(key,ranking_top_times[2].get(key)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file returned by the request is a simple json object with the list of universities ordered by rank after the 'Data' tag. But this time the *Region* is not given by the website, no even on the specific pages. But this time it contains everything else, no need to scrap the specific pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_times = []\n",
    "for uni in ranking_top_times[:200]:\n",
    "    top_times.append({'Name' : uni.get('name'), 'Rank' : uni.get('rank').replace('=', ''), \n",
    "                      'Country' : uni.get('location'), 'Region' : '',\n",
    "                      'Tot_Stud' : int(uni.get('stats_number_students').replace(',', '')), \n",
    "                      'Inter_Stud' : round(int(uni.get('stats_number_students').replace(',', '')) * int(uni.get('stats_pc_intl_students').replace('%', '')) / 100),\n",
    "                      'Tot_Mem' : round(int(uni.get('stats_number_students').replace(',', '')) / float(uni.get('stats_student_staff_ratio'))),\n",
    "                      'Inter_Mem' : None})\n",
    "print(ranking_top_times[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uni in ranking_top_times[:200]:\n",
    "    print(\"***************************************\")\n",
    "    for key in uni.keys(): \n",
    "        print(\"{}: {}\".format(key, uni.get(key)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will save those data in a pickle file to be able to use them without requesting everytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('toptimes.pickle', 'wb') as out:\n",
    "    pickle.dump(top_times, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_times_df = pd.DataFrame.from_dict(top_times).set_index('Rank')\n",
    "top_times_df = top_times_df[['Name', 'Country', 'Region', 'Tot_Stud', 'Inter_Stud', 'Tot_Mem', 'Inter_Mem']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_times_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function prepare the names for the merge. We noticed that in one of the rankings the name is present as well as an abreviation of the university name. For exemple \"Ecole polytechnique federale de lausanne (EPFL)\" and in the othe rankng the abreviation was not present. Because of that we remove all abreviations of that type. We also remove some types of ponctuation and blank spaces.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cleanName(name): \n",
    "    name = name.lower()\n",
    "    name = re.sub(r\"\\(.+\\)\", ' ', name)\n",
    "    name = name.replace('the', '')\n",
    "    name = name.replace('é', '')\n",
    "    name = name.replace('-', ' ')\n",
    "    name = name.replace('—', ' ')\n",
    "    name = name.replace('–', ' ')\n",
    "    name = re.sub(r\"\\s+\", ' ', name)\n",
    "    name = name.strip(' ')\n",
    "    \n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we transform the names of the universities with `cleanName` and we merge the two frames together. We perform an outer join in order to keep university that doesn't have match in the other ranking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_uni_df['Name'] = top_uni_df['Name'].map(lambda name: cleanName(name))\n",
    "top_times_df['Name'] = top_times_df['Name'].map(lambda name: cleanName(name))\n",
    "\n",
    "merged_df = top_times_df.merge(top_uni_df, on=['Name'],  how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[:100]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Task-1.-Compiling-Ebola-Data\"><span class=\"toc-item-num\">Task 1.&nbsp;&nbsp;</span>Compiling Ebola Data</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-2.-RNA-Sequences\"><span class=\"toc-item-num\">Task 2.&nbsp;&nbsp;</span>RNA Sequences</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-3.-Class-War-in-Titanic\"><span class=\"toc-item-num\">Task 3.&nbsp;&nbsp;</span>Class War in Titanic</a></div></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "\n",
    "DATA_FOLDER = 'Data'  # Use the data folder provided in Tutorial 02 - Intro to Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Compiling Ebola Data\n",
    "\n",
    "The `DATA_FOLDER/ebola` folder contains summarized reports of Ebola cases from three countries (Guinea, Liberia and Sierra Leone) during the recent outbreak of the disease in West Africa. For each country, there are daily reports that contain various information about the outbreak in several cities in each country.\n",
    "\n",
    "Use pandas to import these data files into a single `Dataframe`.\n",
    "Using this `DataFrame`, calculate for *each country*, the *daily average per month* of *new cases* and *deaths*.\n",
    "Make sure you handle all the different expressions for *new cases* and *deaths* that are used in the reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell is use to suppress Pandas FutureWarnings which always come up on ambiguity from Column/Index names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guinea\n",
    "\n",
    "### Check structures of the files\n",
    "\n",
    "#### Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFiles = glob.glob(DATA_FOLDER + '/ebola/guinea_data/*.csv')\n",
    "\n",
    "def check_headers(files):\n",
    "    list_ = []\n",
    "    for file_ in files:\n",
    "        df = pd.read_csv(file_, header=None)\n",
    "        list_.append(df.head(n=1))\n",
    "\n",
    "    frame = pd.concat(list_)\n",
    "    # Transpose to not drop record and better visualisation\n",
    "    return frame.drop_duplicates().T\n",
    "\n",
    "check_headers(allFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rows title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Footers, check if the description are the same for each file\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_, usecols=['Description'])\n",
    "    #print(file_, df.shape)\n",
    "    list_.append(df)\n",
    "    \n",
    "frame = pd.concat(list_, axis=1)\n",
    "cleaned = frame.T.drop_duplicates().T\n",
    "first_column = set(cleaned.iloc[:,0].squeeze().tolist())\n",
    "second_column = set(cleaned.iloc[:,1].squeeze().tolist())\n",
    "print(\"sizes: {}\".format(list(map(str, map(len, [first_column, second_column])))))\n",
    "# Extra columns\n",
    "second_column - first_column\n",
    "\n",
    "# Let's discover which files doesn't follow the pattern\n",
    "# Check if data is already the same in each file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Headers:** Therefore, sometimes, region are missing, but the first three columns are always available: Date, Description and Totals. If we see some disrepancies between totals and the sum of all remaining columns, we are going to collect them.\n",
    "\n",
    "**Rows:** \"New deaths registered today\" and \"New deaths registered\" will be considerer as the same data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check date per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_date_unique_per_file():\n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_csv(file_, usecols=['Date'])\n",
    "        #print(file_, df.shape)\n",
    "        if df.drop_duplicates().shape[0] > 1:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "are_date_unique_per_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data\n",
    "\n",
    "Per file, we are going to compare the Totals colun and all region (if they sum up correctly) for the row: \"New deaths registered today (confirmed)\" or \"New deaths registered\" per file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here\n",
    "\n",
    "\n",
    "allFiles = glob.glob(DATA_FOLDER + '/ebola/guinea_data/*.csv')\n",
    "frame = pd.DataFrame()\n",
    "\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_, index_col=['Description', 'Date'], usecols=['Date', 'Description', 'Totals'])\n",
    "    if \"New deaths registered\" in df.index:\n",
    "        print(df.loc[\"New deaths registered among health workers\"])\n",
    "\n",
    "        df = df.loc[\"New deaths registered\"]\n",
    "    else:\n",
    "        df = df.loc[\"New deaths registered today\"]\n",
    "    \n",
    "    list_.append(df)\n",
    "    \n",
    "# New deaths registered among health workers always zero!\n",
    "    \n",
    "    \n",
    "\n",
    "#df[[\"New deaths registered today (confirmed)\"]]\n",
    "#df.loc[df.Description == \"New deaths registered today\"]\n",
    "frame = pd.concat(list_)\n",
    "frame.columns = [\"Deaths (via total)\"]\n",
    "frame \n",
    "\n",
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', 3):\n",
    "#frame.sort_index()\n",
    "#frame.index[0]\n",
    "#rame.index = frame.index.map(lambda x: (\"-\".join(x.split('-')[0:2])))\n",
    "#rame.astype(int).groupby(['Date']).sum()/30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_ = []\n",
    "allFiles = glob.glob(DATA_FOLDER + '/ebola/guinea_data/*.csv')\n",
    "#allFiles = [\"Data/ebola/guinea_data/2014-09-22.csv\"]\n",
    "\n",
    "   \n",
    "# column titles    \n",
    "DEATHS = \"Death\"\n",
    "DEATHS_MANUAL_SUM = \"Death (manual sum)\"\n",
    "NEW_CASES = \"New cases\"\n",
    "NEW_CASES_MANUAL_SUM = \"New cases (manual sum)\"\n",
    "divisible_fields = [DEATHS, DEATHS_MANUAL_SUM, NEW_CASES, NEW_CASES_MANUAL_SUM]\n",
    "\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_, index_col=['Description', 'Date']).fillna(value=0)\n",
    "    \n",
    "    #print(df.dtypes)\n",
    "    \n",
    "    \n",
    "    if \"New deaths registered\" in df.index:\n",
    "        deaths = df.loc[\"New deaths registered\"].copy()\n",
    "    else:\n",
    "        deaths = df.loc[\"New deaths registered today\"].copy()\n",
    "        \n",
    "    new_cases = df.loc[\"New cases of confirmed\"].copy()\n",
    "    \n",
    "    #print(new_cases)\n",
    "\n",
    "    deaths_per_region = pd.to_numeric(deaths.drop(\"Totals\", axis=1).values[0]).sum().astype(int)\n",
    "    new_cases_per_region = pd.to_numeric(new_cases.drop(\"Totals\", axis=1).values[0]).sum().astype(int)\n",
    " \n",
    "    \n",
    "    deaths[DEATHS_MANUAL_SUM] = deaths_per_region\n",
    "    deaths[NEW_CASES] = int(new_cases[\"Totals\"][0])\n",
    "    deaths[NEW_CASES_MANUAL_SUM] = new_cases_per_region\n",
    "\n",
    "\n",
    "    \n",
    "    #deaths = deaths[[\"Totals\", \"Death\"]]\n",
    "    #deaths.columns = [\"Deaths\", \"Deaths (manual sum)\"]\n",
    "    \n",
    "    deaths = deaths[[\"Totals\", DEATHS_MANUAL_SUM, NEW_CASES, NEW_CASES_MANUAL_SUM]]\n",
    "    deaths.columns= [DEATHS, DEATHS_MANUAL_SUM, NEW_CASES, NEW_CASES_MANUAL_SUM]\n",
    "    \n",
    "    deaths[DEATHS] = deaths[DEATHS].astype(int)\n",
    "    \n",
    "    #print(deaths)\n",
    "    \n",
    "    list_.append(deaths)\n",
    "    \n",
    "    \n",
    "frame = pd.concat(list_)\n",
    "frame[\"Country\"] = \"Guinea\"\n",
    "frame[\"Date\"] = pd.to_datetime(frame.index, format=\"%Y-%m-%d\")\n",
    "frame\n",
    "frame.reset_index(drop=True)\n",
    "frame.set_index([\"Country\", \"Date\"])\n",
    "\n",
    "frame[\"Month+Year\"] = frame[\"Date\"].apply(lambda x: x.replace(day=1))\n",
    "frame[\"daysInMonth\"] = frame[\"Month+Year\"].apply(lambda x: int(pd.tslib.monthrange(x.year, x.month)[1]))\n",
    "\n",
    "\n",
    "#frame.groupby(\"Month+Year\").sum() / frame[\"daysInMonth\"]\n",
    "frame[divisible_fields] = frame[divisible_fields].div(frame[\"daysInMonth\"].values,axis=0)\n",
    "frame = frame.drop([\"Month+Year\", \"daysInMonth\", \"Date\"], axis=1)\n",
    "#frame[DEATHS].values\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liberia\n",
    "\n",
    "### Check structures of the files\n",
    "\n",
    "#### Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFiles = glob.glob(DATA_FOLDER + '/ebola/liberia_data/*.csv')\n",
    "\n",
    "check_headers(allFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanted to use first the \"Newly reported deaths\" column, but they were disrepedancies with the cumulated data, and the newly reported data did not indicate if it was confirmed, suspected, etc. For the \"Total death/s in confirmed cases\",  less than half of the files didn't contain the value for this field. So either we switch to no data or more data but overestimation. Then we decided to switch to cumulated \"Total death/s in confirmed, probable, suspected cases\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CASES = \"New case/s (confirmed)\"\n",
    "TOTAL_NEW_CASES = \"Total confirmed cases\"\n",
    "TOTAL_DEATH = \"Total death/s in confirmed, probable, suspected cases\"\n",
    "NEWLY_REPORTED_DEATH = \"Newly reported deaths\"\n",
    "\n",
    "\n",
    "# most of error are extra \\n and spaces\n",
    "#columns = list(map(lambda x: re.sub(\"\\s\\s+\" , \" \", x.replace(\"\\n\", \"\")), df.index.levels[0].values.tolist()))\n",
    "def check_column_in_files(files, column_name, lower_case=False):\n",
    "    #Added lower_case boolean not to rewrite function as Sierre Leone data are lowercased.\n",
    "    cols=['Variable', 'Date']\n",
    "    if lower_case:\n",
    "        cols=['variable', 'date']\n",
    "    \n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_csv(file_, index_col=cols).fillna(value=0)\n",
    "        \n",
    "        #df[\"edited_variable\"] = df.index.levels[0].values\n",
    "        #df[\"edited_variable\"] = \n",
    "        new_values = list(map(lambda x: re.sub(\"\\s\\s+\" , \" \", x.replace(\"\\n\", \"\")), df.index.levels[0].values.tolist()))\n",
    "        df.index = df.index.set_levels(levels=new_values, level=0)\n",
    "        \n",
    "        #print(df.index.levels[0])\n",
    "        \n",
    "        #result = False\n",
    "        result = column_name in df.index\n",
    "            \n",
    "        \n",
    "        \n",
    "        if(column_name == TOTAL_DEATH):\n",
    "            pass\n",
    "            #print(\"{} {}\".format(result, file_))\n",
    "        if not result:\n",
    "            if(column_name == TOTAL_DEATH):\n",
    "                print(df.index.levels[0])\n",
    "            return False\n",
    "    \n",
    "        \n",
    "    return True\n",
    "\n",
    "\n",
    "to_check = [NEW_CASES, TOTAL_DEATH, NEWLY_REPORTED_DEATH, TOTAL_NEW_CASES]\n",
    "list(map(lambda x: check_column_in_files(allFiles, x), to_check))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the both lines are available in each file, however a quick check the files indicates that the cell may be empty.\n",
    "\n",
    "Below, we found out that there is many NA values for the lasts months, only totals are available which mixes confirmed, probable and suspected cases. We have chosen along the way to consider only confirmed cases, so we are going to drop those data. First, let's check how many data we are going to drop.\n",
    "\n",
    "Also, there is discrependencies between New deaths reported and cumulated data.\n",
    "\n",
    "For the new cases, we also are going to use the cumulated data, except for the last file, were it seems that the cumulated data has moved to the new cases column. We patch manually this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `2014-10-04-v142.csv` file, there is two Total deaths line, we keep only the second one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allFiles = glob.glob(DATA_FOLDER + '/ebola/liberia_data/*.csv')\n",
    "\n",
    "list_ = []\n",
    "\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_, index_col=['Variable', 'Date'])\n",
    "    \n",
    "    # Normalize variable column\n",
    "    new_values = list(map(lambda x: re.sub(\"\\s\\s+\" , \" \", x.replace(\"\\n\", \"\")), df.index.levels[0].values.tolist()))\n",
    "    df.index = df.index.set_levels(levels=new_values, level=0)\n",
    "    \n",
    "    data = df[[\"National\"]]\n",
    "\n",
    "    \n",
    "    total_deaths = data.loc[TOTAL_DEATH]\n",
    "    new_cases = data.loc[NEW_CASES]\n",
    "    total_cases = data.loc[TOTAL_NEW_CASES]\n",
    "    \n",
    "    concatenated = pd.concat([total_deaths.tail(n=1), new_cases, total_cases], axis=1)\n",
    "    concatenated.columns = [\"Total death\", \"New cases\", \"New cases (cumul.)\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    list_.append(concatenated)\n",
    "    \n",
    "    \n",
    "    \n",
    "frame = pd.concat(list_)\n",
    "#frame.index = pd.to_datetime(frame.index, format=\"%m/%d/%Y\")\n",
    "\n",
    "#frame.index = frame.index.set_levels([frame.index.levels[0], pd.to_datetime(frame.index.levels[1])])\n",
    "#frame = frame.swaplevel()\n",
    "\n",
    "# keep only NaN value.\n",
    "death_null = frame[frame[\"Total death\"].isnull()].shape[0]\n",
    "death_total = frame.shape[0]\n",
    "\n",
    "cases_null = frame[frame[\"New cases\"].isnull()].shape[0]\n",
    "cases_total = frame.shape[0]\n",
    "print(\"NaN Death ratio {} % ({}/{})\".format(death_null / death_total * 100, death_null, death_total))\n",
    "print(\"NaN New Case ratio {} % ({}/{})\".format(cases_null / cases_total * 100, cases_null, cases_total))\n",
    "\n",
    "frame[\"New cases (cumul.)\"][\"12/9/2014\"] = frame[\"New cases\"][\"12/9/2014\"]\n",
    "#frame[\"New cases (cumul.)\"] = \n",
    "\n",
    "frame.index = pd.to_datetime(frame.index)\n",
    "\n",
    "def remove_outliers(x,y):\n",
    "    #print(x, y, x > y)\n",
    "    if(x > y):\n",
    "        return x\n",
    "    return y\n",
    "\n",
    "frame[\"corr\"] = frame[\"New cases (cumul.)\"].rolling(window=2).apply(lambda x: remove_outliers(x[0], x[1]))\n",
    "\n",
    "frame[\"Month\"] = frame.index.map(lambda x: x.month).astype(int)\n",
    "frame[\"Day\"] = frame.index.map(lambda x: x.day).astype(int)\n",
    "frame.index = [frame[\"Month\"], frame[\"Day\"]]\n",
    "\n",
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', 5):\n",
    "    #print(frame)\n",
    "    \n",
    "a = pd.concat(\n",
    "    [frame.groupby(['Month'])['Day'].min().reset_index(),\n",
    "    frame.groupby(['Month'])['Day'].max().reset_index()]\n",
    ").sort_index()\n",
    "result = pd.merge(a, frame, on=[\"Month\", \"Day\"], how=\"left\")\n",
    "\n",
    "result.index = pd.to_datetime(result[['Month','Day']].apply(lambda x : '{}-{}-2011'.format(x[0],x[1]), axis=1))\n",
    "result = result.drop([\"Month\", \"Day\"], axis=1)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_year_month(x):\n",
    "    return str(x.year) + '-' + str(x.month)\n",
    "\n",
    "def avg_table_daily(table, ):\n",
    "    new_indexes = reversed(list(set(table.index.map(only_year_month))))\n",
    "    avg_result = pd.DataFrame(index=new_indexes, columns=['Total Death (daily avg.)', 'New cases (daily avg.)'])\n",
    "    index_list = table.index\n",
    "    i = 0\n",
    "    while i < len(index_list):\n",
    "        new_ind = str(index_list[i].year) + '-' + str(index_list[i].month)\n",
    "        num_days = (int(index_list[i+1].day) - int(index_list[i].day)) + 1\n",
    "        avg_result['Total Death (daily avg.)'][new_ind] = (int(table['Total death'][index_list[i+1]]) - int(table['Total death'][index_list[i]])) / num_days\n",
    "        avg_result['New cases (daily avg.)'][new_ind] = (int(table['New cases (cumul.)'][index_list[i+1]]) - int(table['New cases (cumul.)'][index_list[i]])) / num_days\n",
    "        i =  i + 2\n",
    "\n",
    "    return avg_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_result_liberia = avg_table_daily(result)\n",
    "avg_result_liberia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sierre Leone\n",
    "\n",
    "### Check structures of the files\n",
    "\n",
    "#### Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allFiles = glob.glob(DATA_FOLDER + '/ebola/sl_data/*.csv')\n",
    "check_headers(allFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Sierra Leone we decided to keep the *confirmed* number for the new cases and death, as it seemed to make more sense to keep only the amount of infected and dead we were sure about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_NEW_CASES = \"cum_confirmed\"\n",
    "TOTAL_DEATH = \"death_confirmed\"\n",
    "\n",
    "to_check = [TOTAL_DEATH, TOTAL_NEW_CASES]\n",
    "list(map(lambda x: check_column_in_files(allFiles, x, True), to_check))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the total deaths data was missing from the last file for december (13-12-2014) we decided to keep the amounts (for every column) shown in the last file that was complete (05-12-2014). This is why we do not take the last two files in the calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_ = []\n",
    "\n",
    "for file_ in allFiles[:len(allFiles) - 2]:\n",
    "    df = pd.read_csv(file_, index_col=['variable', 'date'])\n",
    "    \n",
    "    # Normalize variable column\n",
    "    new_values = list(map(lambda x: re.sub(\"\\s\\s+\" , \" \", x.replace(\"\\n\", \"\")), df.index.levels[0].values.tolist()))\n",
    "    df.index = df.index.set_levels(levels=new_values, level=0)\n",
    "    \n",
    "    data = df[[\"National\"]]\n",
    "    \n",
    "    total_deaths = data.loc[TOTAL_DEATH]\n",
    "    total_cases = data.loc[TOTAL_NEW_CASES]\n",
    "    \n",
    "    concatenated = pd.concat([total_deaths.tail(n=1), total_cases], axis=1)\n",
    "    concatenated.columns = [\"Total death\", \"New cases (cumul.)\"]\n",
    "    \n",
    "    list_.append(concatenated)\n",
    "    \n",
    "    \n",
    "    \n",
    "frame = pd.concat(list_)\n",
    "frame.index = pd.to_datetime(frame.index)\n",
    "frame[\"Month\"] = frame.index.map(lambda x: x.month).astype(int)\n",
    "frame[\"Day\"] = frame.index.map(lambda x: x.day).astype(int)\n",
    "frame.index = [frame[\"Month\"], frame[\"Day\"]]\n",
    "\n",
    "a = pd.concat(\n",
    "    [frame.groupby(['Month'])['Day'].min().reset_index(),\n",
    "    frame.groupby(['Month'])['Day'].max().reset_index()]\n",
    ").sort_index()\n",
    "result = pd.merge(a, frame, on=[\"Month\", \"Day\"], how=\"left\")\n",
    "\n",
    "result.index = pd.to_datetime(result[['Month','Day']].apply(lambda x : '{}-{}-2014'.format(x[0],x[1]), axis=1))\n",
    "result = result.drop([\"Month\", \"Day\"], axis=1)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now putting all that togheter and averaging daily we get this results for the Sierra Leone data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_result_SL = avg_table_daily(result)\n",
    "avg_result_SL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. RNA Sequences\n",
    "\n",
    "In the `DATA_FOLDER/microbiome` subdirectory, there are 9 spreadsheets of microbiome data that was acquired from high-throughput RNA sequencing procedures, along with a 10<sup>th</sup> file that describes the content of each. \n",
    "\n",
    "Use pandas to import the first 9 spreadsheets into a single `DataFrame`.\n",
    "Then, add the metadata information from the 10<sup>th</sup> spreadsheet as columns in the combined `DataFrame`.\n",
    "Make sure that the final `DataFrame` has a unique index and all the `NaN` values have been replaced by the tag `unknown`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this exercice we will first load the metadata and modify the NA value to the *unknown* tag. After that we will load every MIDx.xls file in a separate DataFrame and store them in a list to be able to concat them together. While we load the files we will add a new column call *BARCODE* and store which file every row comes from. This will allow us to use a simple merge to fill the metadatas in for each line. Finaly to keep unique indexes we will use the tupple [*BARCODE, TAXON*]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = DATA_FOLDER + '/microbiome/'\n",
    "METADATA = 'metadata'\n",
    "MID = 'MID'\n",
    "XLS = '.xls'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_excel(DATA_FILE + METADATA + XLS).fillna('unknown')\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df_list = []\n",
    "for x in range(9):\n",
    "    temp_df = pd.read_excel(DATA_FILE + MID + str(x+1) + XLS, header=None)\n",
    "    temp_df['BARCODE'] = pd.Series([MID + str(x+1)]*len(temp_df))\n",
    "    temp_df_list.append(temp_df)\n",
    "taxons = pd.concat(temp_df_list)\n",
    "taxons.columns = ['TAXON', 'COUNT', 'BARCODE']\n",
    "taxons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = taxons.merge(metadata, on='BARCODE', how='left').set_index(['BARCODE', 'TAXON'])\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the index is unique and that there is no more NaN values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Class War in Titanic\n",
    "\n",
    "Use pandas to import the data file `Data/titanic.xls`. It contains data on all the passengers that travelled on the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(filename=DATA_FOLDER+'/titanic.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following questions state clearly your assumptions and discuss your findings:\n",
    "1. Describe the *type* and the *value range* of each attribute. Indicate and transform the attributes that can be `Categorical`. \n",
    "2. Plot histograms for the *travel class*, *embarkation port*, *sex* and *age* attributes. For the latter one, use *discrete decade intervals*. \n",
    "3. Calculate the proportion of passengers by *cabin floor*. Present your results in a *pie chart*.\n",
    "4. For each *travel class*, calculate the proportion of the passengers that survived. Present your results in *pie charts*.\n",
    "5. Calculate the proportion of the passengers that survived by *travel class* and *sex*. Present your results in *a single histogram*.\n",
    "6. Create 2 equally populated *age categories* and calculate survival proportions by *age category*, *travel class* and *sex*. Present your results in a `DataFrame` with unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and cleanup data\n",
    "frame = pd.read_excel(DATA_FOLDER+'/titanic.xls')\n",
    "frame.ticket = frame.ticket.map(lambda tickets: str(tickets).strip('{}{}'.format(string.ascii_letters, string.punctuation)).split(' ')[-1])\n",
    "frame.ticket = frame.ticket.map(lambda tickets: 0 if len(tickets)==0 else int(tickets))\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Describe the *type* and the *value range* of each attribute.\n",
    "\n",
    "print(\"\\npclass \\n range: {}\\n type: {}\".format(frame.pclass.unique(), frame.pclass.dtype))\n",
    "print(\"\\nsurvived \\n range: {}\\n type: {}\".format(frame.survived.unique(), frame.survived.dtype))\n",
    "print(\"\\nsex \\n range: {}\\n type: {}\".format(frame.sex.unique(), frame.sex.dtype))\n",
    "print(\"\\nembarked \\n range: {}\\n type: {}\".format(frame.embarked.unique(), frame.embarked.dtype))\n",
    "print(\"\\nparch \\n range: {}-{}\\n type: {}\".format(frame.parch.min(), frame.parch.max(), frame.parch.dtype))\n",
    "print(\"\\nsibsp \\n range: {}-{}\\n type: {}\".format(frame.sibsp.min(), frame.sibsp.max(), frame.sibsp.dtype))\n",
    "print(\"\\nage \\n range: {}-{}\\n type: {}\".format(frame.age.min(), frame.age.max(), frame.age.dtype))\n",
    "print(\"\\nticket \\n range: {}-{}\\n type: {}\".format(frame.ticket.min(), frame.ticket.max(), frame.ticket.dtype))\n",
    "print(\"\\nfare \\n range: {}-{}\\n type: {}\".format(frame.fare.min(), frame.fare.max(), frame.fare.dtype))\n",
    "print(\"\\nbody \\n range: {}-{}\\n type: {}\".format(frame.body.min(), frame.body.max(), frame.body.dtype))\n",
    "print(\"\\nboat \\n range: {}\\n type: {}\".format(frame.boat.unique(), frame.boat.dtype))\n",
    "print(\"\\ncabin \\n range: {}\\n type: {}\".format(frame.cabin.unique(), frame.cabin.dtype))\n",
    "print(\"\\nname \\n range: {}\\n type: {}\".format(frame.name.unique(), frame.name.dtype))\n",
    "print(\"\\nhome.dest \\n range: {}\\n type: {}\".format(frame['home.dest'].unique(), frame['home.dest'].dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>pclass</b>, <b>survived</b> and <b>sex</b> can be categorical values. These are deviding the dataset to reasonably smaller subsets\n",
    "<b>embarked</b> can be also considered as categorical value if we decide to eliminate the samples that have 'nan' value in this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the attributes that can be `Categorical`. \n",
    "frame['pclass'] = frame.pclass.astype('category')\n",
    "frame['survived'] = frame.survived.astype('category')\n",
    "frame['sex'] = frame.sex.astype('category')\n",
    "frame['embarked'] = frame.embarked.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns that won't be used furhter\n",
    "clean_frame = frame.drop(['sibsp', 'parch', 'ticket', 'fare', 'body', 'home.dest', 'boat', 'name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "#Plot histograms for the *travel class*, *embarkation port*,\n",
    "#*sex* and *age* attributes. For the latter one, use *discrete decade intervals*. \n",
    "\n",
    "clean_frame['sex'].value_counts().plot(kind='bar', color=['r','g'], title='Grouped by sex')\n",
    "plt.show()\n",
    "\n",
    "clean_frame['pclass'].value_counts().plot(kind='bar', color=['r','g','b'], title='Grouped by travel class')\n",
    "plt.show()\n",
    "\n",
    "clean_frame['embarked'].value_counts().plot(kind='bar', color=['r','g','b'], title='Grouped by embarkation port')\n",
    "plt.show()\n",
    "\n",
    "pd.cut(clean_frame.age, [0,10,20,30,40,50,60,70,80]).value_counts().plot(kind='bar', title='Grouped by decade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in clean_frame.cabin.values:\n",
    "    if len(str(c).split(' '))>1:\n",
    "        print(c) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [this](https://www.encyclopedia-titanica.org/titanic-deckplans/g-deck.html) map, we assume that the starting letter for each value represent the floor on which the person stays. For the cases where we have for exemple 'F G63' we assume that the person stayed on floor F in part G cabin 63. When there are more than one value as cabin for the same person we noticed that these cabins are always on the same floor (showed above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the proportion of passengers by cabin floor. Present your results in a pie chart\n",
    "\n",
    "#Add new column named floor\n",
    "clean_frame['floor'] = clean_frame.cabin.dropna().map(lambda f: f[0])\n",
    "#Make it categorical \n",
    "clean_frame['floor'] = clean_frame.floor.astype('category')\n",
    "#Show the piechart\n",
    "clean_frame['floor'].value_counts().plot(kind='pie', title='Grouped by cabin floor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each travel class, calculate the proportion of the passengers that survived.\n",
    "#Present your results in pie charts.\n",
    "clean_frame[clean_frame.survived==1]['pclass'].value_counts().plot(kind='pie', title='Survived proportions from each class')\n",
    "plt.show()\n",
    "clean_frame.groupby(['pclass', 'survived']).pclass.value_counts().plot(kind='pie', title='Survived(1) vs. Dead(0) all together')\n",
    "plt.show()\n",
    "clean_frame[clean_frame.pclass==1]['survived'].value_counts().plot(kind='pie', title='Survived(1) from class 1')\n",
    "plt.show()\n",
    "clean_frame[clean_frame.pclass==2]['survived'].value_counts().plot(kind='pie', title='Survived(1) from class 2')\n",
    "plt.show()\n",
    "clean_frame[clean_frame.pclass==3]['survived'].value_counts().plot(kind='pie', title='Survived(1) from class 3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the proportion of the passengers that survived by travel class and sex.\n",
    "#Present your results in a single histogram.\n",
    "grouped = clean_frame[clean_frame.survived==1].groupby(['pclass', 'sex'])\n",
    "\n",
    "grouped.survived.value_counts().plot(kind='bar', color=['r','g','b','m','y', 'c'], title='Survived passengers by sex and travel class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 2 equally populated age categories and calculate survival proportions by age category, travel class and sex. \n",
    "by_ages = pd.qcut(clean_frame.age, 2)\n",
    "clean_frame['by_ages'] = by_ages\n",
    "\n",
    "grouped = clean_frame[clean_frame.survived==1].groupby(['pclass', 'sex', 'by_ages'])\n",
    "grouped.survived.value_counts().plot(kind='bar', color=['r','g','b','m','y', 'c'], title='Survived passengers by sex, travel class and age category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Present your results in a DataFrame with unique index.\n",
    "clean_frame['id'] = range(len(clean_frame))\n",
    "clean_frame = clean_frame.set_index(['id'])\n",
    "clean_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

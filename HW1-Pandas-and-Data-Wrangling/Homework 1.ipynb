{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Task-1.-Compiling-Ebola-Data\"><span class=\"toc-item-num\">Task 1.&nbsp;&nbsp;</span>Compiling Ebola Data</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-2.-RNA-Sequences\"><span class=\"toc-item-num\">Task 2.&nbsp;&nbsp;</span>RNA Sequences</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-3.-Class-War-in-Titanic\"><span class=\"toc-item-num\">Task 3.&nbsp;&nbsp;</span>Class War in Titanic</a></div></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "\n",
    "DATA_FOLDER = 'Data'  # Use the data folder provided in Tutorial 02 - Intro to Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Compiling Ebola Data\n",
    "\n",
    "The `DATA_FOLDER/ebola` folder contains summarized reports of Ebola cases from three countries (Guinea, Liberia and Sierra Leone) during the recent outbreak of the disease in West Africa. For each country, there are daily reports that contain various information about the outbreak in several cities in each country.\n",
    "\n",
    "Use pandas to import these data files into a single `Dataframe`.\n",
    "Using this `DataFrame`, calculate for *each country*, the *daily average per month* of *new cases* and *deaths*.\n",
    "Make sure you handle all the different expressions for *new cases* and *deaths* that are used in the reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell is use to suppress Pandas FutureWarnings which always come up on ambiguity from Column/Index names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to Format result tables from Dataframe.\n",
    "\n",
    "We'll explain later why we use them. But we use cumulated data (which are increasing along the month). So what we do in this function is to divide the earliest and the latest data of each month by the number of elapsing days. From all the data point there is on each file.\n",
    "\n",
    "To do so we have to update index to a sortable value representing only month and year (integer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_DEATH_DAILY_AVG = 'Total Death (daily avg.)'\n",
    "NEW_CASES_DAILY_AVG = 'New cases (daily avg.)'\n",
    "\n",
    "# We create a numerical index to sort by year month.\n",
    "def only_year_month(x):\n",
    "    return x.year * 100 + x.month\n",
    "\n",
    "def avg_table_daily(table):\n",
    "    new_indexes = reversed(list(set(table.index.map(only_year_month))))\n",
    "    avg_result = pd.DataFrame(index=new_indexes, columns=[TOTAL_DEATH_DAILY_AVG, NEW_CASES_DAILY_AVG])\n",
    "    old_index = table.index\n",
    "    i = 0\n",
    "    while i < len(old_index):\n",
    "        new_ind = only_year_month(old_index[i])\n",
    "        num_days = old_index[i+1].day - old_index[i].day + 1\n",
    "        \n",
    "        [x,y] = table['Total death'][old_index[i]:old_index[i+1]]\n",
    "        avg_result[TOTAL_DEATH_DAILY_AVG][new_ind] = (y-x) / num_days\n",
    "        \n",
    "        [x,y] = table['New cases (cumul.)'][old_index[i]:old_index[i+1]]\n",
    "        avg_result[NEW_CASES_DAILY_AVG][new_ind] = (y-x) / num_days\n",
    "        i =  i + 2\n",
    "\n",
    "    avg_result = avg_result.sort_index()\n",
    "        \n",
    "    return avg_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to check headers from a list of files. It display set of different headers to be able to visually check if columns are missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_headers(files):\n",
    "    list_ = []\n",
    "    for file_ in files:\n",
    "        df = pd.read_csv(file_, header=None)\n",
    "        list_.append(df.head(n=1))\n",
    "\n",
    "    frame = pd.concat(list_)\n",
    "    # Transpose to not drop record and better visualisation\n",
    "    return frame.drop_duplicates().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function checking if dates are unique per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def are_date_unique_per_file(files, lowercase=False):\n",
    "    cols=['Date']\n",
    "    if lowercase:\n",
    "        cols=['date']\n",
    "    for file_ in files:\n",
    "        df = pd.read_csv(file_, usecols=cols)\n",
    "        #print(file_, df.shape)\n",
    "        if df.drop_duplicates().shape[0] > 1:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liberia\n",
    "\n",
    "### Check structures of the files\n",
    "\n",
    "#### Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allFiles = glob.glob(DATA_FOLDER + '/ebola/liberia_data/*.csv')\n",
    "\n",
    "check_headers(allFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "are_date_unique_per_file(allFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanted to use first the \"Newly reported deaths\" column, but they were disrepedancies with the cumulated data, and the newly reported data did not indicate if it was confirmed, suspected, etc. For the \"Total death/s in confirmed cases\",  less than half of the files didn't contain the value for this field. So either we switch to no data or more data but overestimation. Then we decided to switch to cumulated \"Total death/s in confirmed, probable, suspected cases\".\n",
    "\n",
    "We also normalize most of the column by replacing new lines and repeating space since this is what we spotted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CASES = \"New case/s (confirmed)\"\n",
    "TOTAL_NEW_CASES = \"Total confirmed cases\"\n",
    "TOTAL_DEATH = \"Total death/s in confirmed, probable, suspected cases\"\n",
    "NEWLY_REPORTED_DEATH = \"Newly reported deaths\"\n",
    "\n",
    "\n",
    "# most of error are extra \\n and spaces\n",
    "def check_column_in_files(files, column_name, lower_case=False, descript=False):\n",
    "    # Added lower_case boolean not to rewrite function as Sierre Leone data are lowercased.\n",
    "    # Added descript boolean not to rewrite function as Guinea data are different.\n",
    "    cols=['Variable', 'Date']\n",
    "    if lower_case:\n",
    "        cols=['variable', 'date']\n",
    "    if descript:\n",
    "        cols=['Description', 'Date']\n",
    "    \n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_csv(file_, index_col=cols).fillna(value=0)\n",
    "        \n",
    "        # normalize column names\n",
    "        new_values = list(map(lambda x: re.sub(\"\\s\\s+\" , \" \", x.replace(\"\\n\", \"\")), df.index.levels[0].values.tolist()))\n",
    "        df.index = df.index.set_levels(levels=new_values, level=0)\n",
    "    \n",
    "        is_column_present = column_name in df.index\n",
    "        \n",
    "        if not is_column_present:\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "\n",
    "to_check = [NEW_CASES, TOTAL_DEATH, NEWLY_REPORTED_DEATH, TOTAL_NEW_CASES]\n",
    "list(map(lambda x: check_column_in_files(allFiles, x), to_check))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the both lines are available in each file, however a quick check in the files indicates that the cell may be empty.\n",
    "\n",
    "Below, we found out that there is many NA values for the lasts months, only totals are available which mixes confirmed, probable and suspected cases. We have chosen along the way to consider only *confirmed cases*, so we are going to drop those data. First, let's check how many data we are going to drop.\n",
    "\n",
    "Also, there is discrependencies between New deaths reported and cumulated data.\n",
    "\n",
    "For the new cases, we also are going to use the cumulated data, except for the last file, were it seems that the cumulated data has moved to the new cases column. We patch manually this.\n",
    "\n",
    "## More about cumulated data\n",
    "\n",
    "We pick the earliest and the latest day in each month and we do the difference between the two to compute the average death and new cases per month. What happen is that the latest and the earliest day in the month do not match respectively with the end and the beginning of the month. We have two options from there. Assuming with have data from the 4th September to the 25th. And next month the data are starting at the 3rd of October. Either we don't take in account the gap or we try to interpolate the data. But we may not know the distribution of the new cases/death between each day so we found it was dangerous to interpolate. We will only do the averages on the data we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `2014-10-04-v142.csv` file, there is two Total deaths line, we keep only the second one (because it is the largest)\n",
    "\n",
    "For the cumulated data, they are not always increasing, and we apply a `rolling` operation to correct values that decrease by replacing the ovbviously wrong value (nobody resuscitates!) by its precendent value. It may be incorrect but still better than discarding a value. We checked that it doesn't happen at the end or the beginning of a month. Rolling discard the very first and very last value, so we manually replace them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allFiles = glob.glob(DATA_FOLDER + '/ebola/liberia_data/*.csv')\n",
    "\n",
    "list_ = []\n",
    "\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_, index_col=['Variable', 'Date'])\n",
    "    \n",
    "    # Normalize variable column\n",
    "    new_values = list(map(lambda x: re.sub(\"\\s\\s+\" , \" \", x.replace(\"\\n\", \"\")), df.index.levels[0].values.tolist()))\n",
    "    df.index = df.index.set_levels(levels=new_values, level=0)\n",
    "    \n",
    "    data = df[[\"National\"]]\n",
    "\n",
    "    total_deaths = data.loc[TOTAL_DEATH]\n",
    "    new_cases = data.loc[NEW_CASES]\n",
    "    total_cases = data.loc[TOTAL_NEW_CASES]\n",
    "    \n",
    "    concatenated = pd.concat([total_deaths.tail(n=1), new_cases, total_cases], axis=1)\n",
    "    concatenated.columns = [\"Total death\", \"New cases\", \"New cases (cumul.)\"]\n",
    "    list_.append(concatenated)\n",
    "    \n",
    "frame = pd.concat(list_)\n",
    "\n",
    "# Manual patch\n",
    "frame[\"New cases (cumul.)\"][\"12/9/2014\"] = frame[\"New cases\"][\"12/9/2014\"]\n",
    "frame.index = pd.to_datetime(frame.index)\n",
    "\n",
    "# To remove decreasing number in an increasing serie, we compare two number in\n",
    "# a moving window fashion and replace a number with its previous row content\n",
    "# if it is smaller (since cumulated value should only increase)\n",
    "def remove_outliers(x,y):\n",
    "    if(x > y):\n",
    "        return x\n",
    "    return y\n",
    "\n",
    "frame[\"corr\"] = frame[\"New cases (cumul.)\"].rolling(window=2).apply(lambda x: remove_outliers(x[0], x[1]))\n",
    "\n",
    "# We keep only the first \n",
    "frame[\"Month\"] = frame.index.map(lambda x: x.month).astype(int)\n",
    "frame[\"Day\"] = frame.index.map(lambda x: x.day).astype(int)\n",
    "frame.index = [frame[\"Month\"], frame[\"Day\"]]\n",
    "\n",
    "a = pd.concat(\n",
    "    [frame.groupby(['Month'])['Day'].min().reset_index(),\n",
    "    frame.groupby(['Month'])['Day'].max().reset_index()]\n",
    ").sort_index()\n",
    "result = pd.merge(a, frame, on=[\"Month\", \"Day\"], how=\"left\")\n",
    "\n",
    "result.index = pd.to_datetime(result[['Month','Day']].apply(lambda x : '{}-{}-2014'.format(x[0],x[1]), axis=1))\n",
    "result = result.drop([\"Month\", \"Day\"], axis=1)\n",
    "\n",
    "result.loc[\"2014-06-16\"][\"corr\"] = result.loc[\"2014-06-16\"][\"New cases (cumul.)\"]\n",
    "result.loc[\"2014-12-09\"][\"corr\"] = result.loc[\"2014-12-09\"][\"New cases (cumul.)\"]\n",
    "result = result.drop([\"New cases (cumul.)\", \"New cases\"], axis=1)\n",
    "result = result.rename(columns = {'corr':'New cases (cumul.)'})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained before we will only consider the cumulative results as they are much more coherent et complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now putting all that togheter and averaging daily we get this results for the Liberia data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_result_L = avg_table_daily(result)\n",
    "avg_result_L['Country'] = 'Liberia'\n",
    "avg_result_L.set_index(\"Country\", append=True)\n",
    "avg_result_L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sierre Leone\n",
    "\n",
    "### Check structures of the files\n",
    "\n",
    "#### Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allFiles = glob.glob(DATA_FOLDER + '/ebola/sl_data/*.csv')\n",
    "check_headers(allFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "are_date_unique_per_file(allFiles, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Sierra Leone we decided to keep the *confirmed* number for the new cases and death, as it seemed to make more sense to keep only the amount of infected and dead we were sure about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_NEW_CASES = \"cum_confirmed\"\n",
    "TOTAL_DEATH = \"death_confirmed\"\n",
    "\n",
    "to_check = [TOTAL_DEATH, TOTAL_NEW_CASES]\n",
    "list(map(lambda x: check_column_in_files(allFiles, x, True, False), to_check))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the total deaths data was missing from the last file for december (13-12-2014) we decided to keep the amounts (for every column) shown in the last file that was complete (05-12-2014). This is why we do not take the last two files in the calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_ = []\n",
    "\n",
    "for file_ in allFiles[:len(allFiles) - 2]:\n",
    "    df = pd.read_csv(file_, index_col=['variable', 'date'], thousands=',')\n",
    "    \n",
    "    # Normalize variable column\n",
    "    new_values = list(map(lambda x: re.sub(\"\\s\\s+\" , \" \", x.replace(\"\\n\", \"\")), df.index.levels[0].values.tolist()))\n",
    "    df.index = df.index.set_levels(levels=new_values, level=0)\n",
    "    \n",
    "    data = df[[\"National\"]]\n",
    "    \n",
    "    # Remove commas in the series\n",
    "    total_deaths = data.loc[TOTAL_DEATH].replace(regex=True,to_replace=r',',value=r'').fillna(value=0).astype(int)\n",
    "    total_cases = data.loc[TOTAL_NEW_CASES].replace(regex=True,to_replace=r',',value=r'').fillna(value=0).astype(int)\n",
    "        \n",
    "    concatenated = pd.concat([total_deaths.tail(n=1), total_cases], axis=1)\n",
    "    concatenated.columns = [\"Total death\", \"New cases (cumul.)\"]\n",
    "    \n",
    "    list_.append(concatenated)\n",
    "    \n",
    "    \n",
    "    \n",
    "frame = pd.concat(list_)\n",
    "frame.index = pd.to_datetime(frame.index)\n",
    "frame[\"Month\"] = frame.index.map(lambda x: x.month).astype(int)\n",
    "frame[\"Day\"] = frame.index.map(lambda x: x.day).astype(int)\n",
    "frame.index = [frame[\"Month\"], frame[\"Day\"]]\n",
    "\n",
    "a = pd.concat(\n",
    "    [frame.groupby(['Month'])['Day'].min().reset_index(),\n",
    "    frame.groupby(['Month'])['Day'].max().reset_index()]\n",
    ").sort_index()\n",
    "result = pd.merge(a, frame, on=[\"Month\", \"Day\"], how=\"left\")\n",
    "\n",
    "result.index = pd.to_datetime(result[['Month','Day']].apply(lambda x : '{}-{}-2014'.format(x[0],x[1]), axis=1))\n",
    "result = result.drop([\"Month\", \"Day\"], axis=1)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now putting all that togheter and averaging daily we get this results for the Sierra Leone data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_result_SL = avg_table_daily(result)\n",
    "avg_result_SL['Country'] = 'Sierra Leone'\n",
    "avg_result_SL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guinea\n",
    "\n",
    "### Check structures of the files\n",
    "\n",
    "#### Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allFiles = glob.glob(DATA_FOLDER + '/ebola/guinea_data/*.csv')\n",
    "check_headers(allFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "are_date_unique_per_file(allFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that three columns are always available: Date, Description and Totals. We decided to use the total cumulative confirmed amount for deaths and new cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_NEW_CASES = \"Total cases of confirmed\"\n",
    "TOTAL_DEATH = \"Total deaths of confirmed\"\n",
    "\n",
    "to_check = [TOTAL_DEATH, TOTAL_NEW_CASES]\n",
    "list(map(lambda x: check_column_in_files(allFiles, x, False, True), to_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_ = []\n",
    "\n",
    "for file_ in allFiles[:len(allFiles)-1]:\n",
    "    df = pd.read_csv(file_, index_col=['Description', 'Date'])\n",
    "    \n",
    "    # Normalize variable column\n",
    "    new_values = list(map(lambda x: re.sub(\"\\s\\s+\" , \" \", x.replace(\"\\n\", \"\")), df.index.levels[0].values.tolist()))\n",
    "    df.index = df.index.set_levels(levels=new_values, level=0)\n",
    "    \n",
    "    data = df[[\"Totals\"]]\n",
    "    \n",
    "    total_deaths = data.loc[TOTAL_DEATH].fillna(value=0).astype(int)\n",
    "    total_cases = data.loc[TOTAL_NEW_CASES].fillna(value=0).astype(int)\n",
    "    \n",
    "    concatenated = pd.concat([total_deaths.tail(n=1), total_cases], axis=1)\n",
    "    concatenated.columns = [\"Total death\", \"New cases (cumul.)\"]\n",
    "    \n",
    "    list_.append(concatenated)\n",
    "    \n",
    "    \n",
    "    \n",
    "frame = pd.concat(list_)\n",
    "frame.index = pd.to_datetime(frame.index)\n",
    "frame[\"Month\"] = frame.index.map(lambda x: x.month).astype(int)\n",
    "frame[\"Day\"] = frame.index.map(lambda x: x.day).astype(int)\n",
    "frame.index = [frame[\"Month\"], frame[\"Day\"]]\n",
    "\n",
    "a = pd.concat(\n",
    "    [frame.groupby(['Month'])['Day'].min().reset_index(),\n",
    "    frame.groupby(['Month'])['Day'].max().reset_index()]\n",
    ").sort_index()\n",
    "result = pd.merge(a, frame, on=[\"Month\", \"Day\"], how=\"left\")\n",
    "\n",
    "result.index = pd.to_datetime(result[['Month','Day']].apply(lambda x : '{}-{}-2014'.format(x[0],x[1]), axis=1))\n",
    "result = result.drop([\"Month\", \"Day\"], axis=1)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The guinea data for October 2014 only contained one day of data. We decided to drop it as it doesn't make much sense to return a daily average per month on one day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now putting all that togheter and averaging daily we get this results for the Guinea data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_result_G = avg_table_daily(result)\n",
    "avg_result_G['Country'] = 'Guinea'\n",
    "avg_result_G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def yearmonth_int_to_string(x):\n",
    "    return \"{}-{}\".format(x // 100, x % 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_result = pd.concat([avg_result_L, avg_result_SL, avg_result_G])\n",
    "total_result = total_result.set_index([total_result['Country'], total_result.index]).drop('Country', axis=1)\n",
    "total_result.index.set_levels(total_result.index.levels[1].map(yearmonth_int_to_string), 1, inplace=True)\n",
    "total_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. RNA Sequences\n",
    "\n",
    "In the `DATA_FOLDER/microbiome` subdirectory, there are 9 spreadsheets of microbiome data that was acquired from high-throughput RNA sequencing procedures, along with a 10<sup>th</sup> file that describes the content of each. \n",
    "\n",
    "Use pandas to import the first 9 spreadsheets into a single `DataFrame`.\n",
    "Then, add the metadata information from the 10<sup>th</sup> spreadsheet as columns in the combined `DataFrame`.\n",
    "Make sure that the final `DataFrame` has a unique index and all the `NaN` values have been replaced by the tag `unknown`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this exercice we will first load the metadata and modify the NA value to the *unknown* tag. After that we will load every MIDx.xls file in a separate DataFrame and store them in a list to be able to concat them together. While we load the files we will add a new column call *BARCODE* and store which file every row comes from. This will allow us to use a simple merge to fill the metadatas in for each line. Finaly to keep unique indexes we will use the tupple [*BARCODE, TAXON*]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FILE = DATA_FOLDER + '/microbiome/'\n",
    "METADATA = 'metadata'\n",
    "MID = 'MID'\n",
    "XLS = '.xls'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata = pd.read_excel(DATA_FILE + METADATA + XLS).fillna('unknown')\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df_list = []\n",
    "for x in range(9):\n",
    "    temp_df = pd.read_excel(DATA_FILE + MID + str(x+1) + XLS, header=None)\n",
    "    temp_df['BARCODE'] = pd.Series([MID + str(x+1)]*len(temp_df))\n",
    "    temp_df_list.append(temp_df)\n",
    "taxons = pd.concat(temp_df_list)\n",
    "taxons.columns = ['TAXON', 'COUNT', 'BARCODE']\n",
    "taxons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = taxons.merge(metadata, on='BARCODE', how='left').set_index(['BARCODE', 'TAXON'])\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the index is unique and that there is no more NaN values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_null = final.isnull()\n",
    "is_null[(is_null[\"COUNT\"]) | (is_null[\"GROUP\"]) | (is_null[\"SAMPLE\"])].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Class War in Titanic\n",
    "\n",
    "Use pandas to import the data file `Data/titanic.xls`. It contains data on all the passengers that travelled on the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HTML(filename=DATA_FOLDER+'/titanic.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following questions state clearly your assumptions and discuss your findings:\n",
    "1. Describe the *type* and the *value range* of each attribute. Indicate and transform the attributes that can be `Categorical`. \n",
    "2. Plot histograms for the *travel class*, *embarkation port*, *sex* and *age* attributes. For the latter one, use *discrete decade intervals*. \n",
    "3. Calculate the proportion of passengers by *cabin floor*. Present your results in a *pie chart*.\n",
    "4. For each *travel class*, calculate the proportion of the passengers that survived. Present your results in *pie charts*.\n",
    "5. Calculate the proportion of the passengers that survived by *travel class* and *sex*. Present your results in *a single histogram*.\n",
    "6. Create 2 equally populated *age categories* and calculate survival proportions by *age category*, *travel class* and *sex*. Present your results in a `DataFrame` with unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load and cleanup data\n",
    "frame = pd.read_excel(DATA_FOLDER+'/titanic.xls')\n",
    "frame.ticket = frame.ticket.map(lambda tickets: str(tickets).strip('{}{}'.format(string.ascii_letters, string.punctuation)).split(' ')[-1])\n",
    "frame.ticket = frame.ticket.map(lambda tickets: 0 if len(tickets)==0 else int(tickets))\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Describe the *type* and the *value range* of each attribute.\n",
    "\n",
    "print(\"\\npclass \\n range: {}\\n type: {}\".format(frame.pclass.unique(), frame.pclass.dtype))\n",
    "print(\"\\nsurvived \\n range: {}\\n type: {}\".format(frame.survived.unique(), frame.survived.dtype))\n",
    "print(\"\\nsex \\n range: {}\\n type: {}\".format(frame.sex.unique(), frame.sex.dtype))\n",
    "print(\"\\nembarked \\n range: {}\\n type: {}\".format(frame.embarked.unique(), frame.embarked.dtype))\n",
    "print(\"\\nparch \\n range: {}-{}\\n type: {}\".format(frame.parch.min(), frame.parch.max(), frame.parch.dtype))\n",
    "print(\"\\nsibsp \\n range: {}-{}\\n type: {}\".format(frame.sibsp.min(), frame.sibsp.max(), frame.sibsp.dtype))\n",
    "print(\"\\nage \\n range: {}-{}\\n type: {}\".format(frame.age.min(), frame.age.max(), frame.age.dtype))\n",
    "print(\"\\nticket \\n range: {}-{}\\n type: {}\".format(frame.ticket.min(), frame.ticket.max(), frame.ticket.dtype))\n",
    "print(\"\\nfare \\n range: {}-{}\\n type: {}\".format(frame.fare.min(), frame.fare.max(), frame.fare.dtype))\n",
    "print(\"\\nbody \\n range: {}-{}\\n type: {}\".format(frame.body.min(), frame.body.max(), frame.body.dtype))\n",
    "print(\"\\nboat \\n range: {}\\n type: {}\".format(frame.boat.unique(), frame.boat.dtype))\n",
    "print(\"\\ncabin \\n range: {}\\n type: {}\".format(frame.cabin.unique(), frame.cabin.dtype))\n",
    "print(\"\\nname \\n range: {}\\n type: {}\".format(frame.name.unique(), frame.name.dtype))\n",
    "print(\"\\nhome.dest \\n range: {}\\n type: {}\".format(frame['home.dest'].unique(), frame['home.dest'].dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>pclass</b>, <b>survived</b> and <b>sex</b> can be categorical values. These are deviding the dataset to reasonably smaller subsets\n",
    "<b>embarked</b> can be also considered as categorical value if we decide to eliminate the samples that have 'nan' value in this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transform the attributes that can be `Categorical`. \n",
    "frame['pclass'] = frame.pclass.astype('category')\n",
    "frame['survived'] = frame.survived.astype('category')\n",
    "frame['sex'] = frame.sex.astype('category')\n",
    "frame['embarked'] = frame.embarked.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Drop columns that won't be used furhter\n",
    "clean_frame = frame.drop(['sibsp', 'parch', 'ticket', 'fare', 'body', 'home.dest', 'boat', 'name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "#Plot histograms for the *travel class*, *embarkation port*,\n",
    "#*sex* and *age* attributes. For the latter one, use *discrete decade intervals*. \n",
    "\n",
    "clean_frame['sex'].value_counts().plot(kind='bar', color=['r','g'], title='Grouped by sex')\n",
    "plt.show()\n",
    "\n",
    "clean_frame['pclass'].value_counts().plot(kind='bar', color=['r','g','b'], title='Grouped by travel class')\n",
    "plt.show()\n",
    "\n",
    "clean_frame['embarked'].value_counts().plot(kind='bar', color=['r','g','b'], title='Grouped by embarkation port')\n",
    "plt.show()\n",
    "\n",
    "pd.cut(clean_frame.age, [0,10,20,30,40,50,60,70,80]).value_counts().plot(kind='bar', title='Grouped by decade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in clean_frame.cabin.values:\n",
    "    if len(str(c).split(' '))>1:\n",
    "        print(c) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [this](https://www.encyclopedia-titanica.org/titanic-deckplans/g-deck.html) map, we assume that the starting letter for each value represent the floor on which the person stays. For the cases where we have for exemple 'F G63' we assume that the person stayed on floor F in part G cabin 63. When there are more than one value as cabin for the same person we noticed that these cabins are always on the same floor (showed above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate the proportion of passengers by cabin floor. Present your results in a pie chart\n",
    "\n",
    "#Add new column named floor\n",
    "clean_frame['floor'] = clean_frame.cabin.dropna().map(lambda f: f[0])\n",
    "#Make it categorical \n",
    "clean_frame['floor'] = clean_frame.floor.astype('category')\n",
    "#Show the piechart\n",
    "clean_frame['floor'].value_counts().plot(kind='pie', title='Grouped by cabin floor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For each travel class, calculate the proportion of the passengers that survived.\n",
    "#Present your results in pie charts.\n",
    "clean_frame[clean_frame.survived==1]['pclass'].value_counts().plot(kind='pie', title='Survived proportions from each class')\n",
    "plt.show()\n",
    "clean_frame.groupby(['pclass', 'survived']).pclass.value_counts().plot(kind='pie', title='Survived(1) vs. Dead(0) all together')\n",
    "plt.show()\n",
    "clean_frame[clean_frame.pclass==1]['survived'].value_counts().plot(kind='pie', title='Survived(1) from class 1')\n",
    "plt.show()\n",
    "clean_frame[clean_frame.pclass==2]['survived'].value_counts().plot(kind='pie', title='Survived(1) from class 2')\n",
    "plt.show()\n",
    "clean_frame[clean_frame.pclass==3]['survived'].value_counts().plot(kind='pie', title='Survived(1) from class 3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate the proportion of the passengers that survived by travel class and sex.\n",
    "#Present your results in a single histogram.\n",
    "grouped = clean_frame[clean_frame.survived==1].groupby(['pclass', 'sex'])\n",
    "\n",
    "grouped.survived.value_counts().plot(kind='bar', color=['r','g','b','m','y', 'c'], title='Survived passengers by sex and travel class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create 2 equally populated age categories and calculate survival proportions by age category, travel class and sex. \n",
    "by_ages = pd.qcut(clean_frame.age, 2)\n",
    "clean_frame['by_ages'] = by_ages\n",
    "\n",
    "grouped = clean_frame[clean_frame.survived==1].groupby(['pclass', 'sex', 'by_ages'])\n",
    "grouped.survived.value_counts().plot(kind='bar', color=['r','g','b','m','y', 'c'], title='Survived passengers by sex, travel class and age category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Present your results in a DataFrame with unique index.\n",
    "clean_frame['id'] = range(len(clean_frame))\n",
    "clean_frame = clean_frame.set_index(['id'])\n",
    "clean_frame"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
